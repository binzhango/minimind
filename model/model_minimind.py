# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Config
#                                             MiniMind é…ç½®ç±»
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

# å¯¼å…¥transformersåº“ä¸­çš„é¢„è®­ç»ƒé…ç½®åŸºç±»
from transformers import PretrainedConfig


# MiniMindæ¨¡å‹çš„é…ç½®ç±»ï¼Œç»§æ‰¿è‡ªPretrainedConfig
# è¿™ä¸ªç±»ç”¨äºå­˜å‚¨æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°ï¼ˆhyperparametersï¼‰
class MiniMindConfig(PretrainedConfig):
    # å®šä¹‰æ¨¡å‹ç±»å‹ä¸º"minimind"ï¼Œç”¨äºåœ¨transformersåº“ä¸­è¯†åˆ«è¿™ä¸ªæ¨¡å‹
    model_type = "minimind"

    # åˆå§‹åŒ–å‡½æ•°ï¼Œå®šä¹‰æ¨¡å‹çš„æ‰€æœ‰é…ç½®å‚æ•°
    def __init__(
            self,
            dropout: float = 0.0,  # dropoutæ¯”ä¾‹ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œ0.0è¡¨ç¤ºä¸ä½¿ç”¨dropout
            bos_token_id: int = 1,  # å¥å­å¼€å§‹æ ‡è®°(Beginning Of Sentence)çš„ID
            eos_token_id: int = 2,  # å¥å­ç»“æŸæ ‡è®°(End Of Sentence)çš„ID
            hidden_act: str = 'silu',  # æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œsiluæ˜¯ä¸€ç§å¹³æ»‘çš„æ¿€æ´»å‡½æ•°
            hidden_size: int = 512,  # éšè—å±‚çš„ç»´åº¦å¤§å°ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªè¯å‘é‡çš„é•¿åº¦
            intermediate_size: int = None,  # å‰é¦ˆç½‘ç»œä¸­é—´å±‚çš„å¤§å°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
            max_position_embeddings: int = 32768,  # æœ€å¤§ä½ç½®ç¼–ç æ•°ï¼Œå³æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦
            num_attention_heads: int = 8,  # æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œç”¨äºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
            num_hidden_layers: int = 8,  # Transformerå±‚çš„æ•°é‡ï¼Œå³æ¨¡å‹çš„æ·±åº¦
            num_key_value_heads: int = 2,  # KVç¼“å­˜çš„å¤´æ•°ï¼Œç”¨äºåŠ é€Ÿæ¨ç†
            vocab_size: int = 6400,  # è¯æ±‡è¡¨å¤§å°ï¼Œå³æ¨¡å‹èƒ½è¯†åˆ«çš„ä¸åŒè¯çš„æ•°é‡
            rms_norm_eps: float = 1e-05,  # RMSNormå½’ä¸€åŒ–çš„epsilonå€¼ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯
            rope_theta: int = 1000000.0,  # RoPEä½ç½®ç¼–ç çš„thetaå‚æ•°ï¼Œæ§åˆ¶ä½ç½®ç¼–ç çš„é¢‘ç‡
            flash_attn: bool = True,  # æ˜¯å¦ä½¿ç”¨Flash AttentionåŠ é€Ÿè®¡ç®—
            ####################################################
            # ä»¥ä¸‹æ˜¯æ··åˆä¸“å®¶æ¨¡å‹(MoE)çš„ç‰¹å®šé…ç½®
            # å½“use_moeä¸ºFalseæ—¶ï¼Œä»¥ä¸‹å‚æ•°æ— æ•ˆ
            ####################################################
            use_moe: bool = False,  # æ˜¯å¦ä½¿ç”¨æ··åˆä¸“å®¶æ¨¡å‹
            num_experts_per_tok: int = 2,  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
            n_routed_experts: int = 4,  # å¯è·¯ç”±çš„ä¸“å®¶æ€»æ•°
            n_shared_experts: int = 1,  # å…±äº«ä¸“å®¶çš„æ•°é‡ï¼ˆæ‰€æœ‰tokenéƒ½ä¼šä½¿ç”¨ï¼‰
            scoring_func: str = 'softmax',  # ä¸“å®¶é€‰æ‹©çš„è¯„åˆ†å‡½æ•°
            aux_loss_alpha: float = 0.1,  # è¾…åŠ©æŸå¤±çš„æƒé‡ç³»æ•°
            seq_aux: bool = True,  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤±
            norm_topk_prob: bool = True,  # æ˜¯å¦å¯¹top-kæ¦‚ç‡è¿›è¡Œå½’ä¸€åŒ–
            **kwargs  # å…¶ä»–é¢å¤–å‚æ•°
    ):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–å‡½æ•°
        super().__init__(**kwargs)
        
        # å°†æ‰€æœ‰å‚æ•°ä¿å­˜ä¸ºç±»çš„å±æ€§ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
        self.dropout = dropout  # ä¿å­˜dropoutæ¯”ä¾‹
        self.bos_token_id = bos_token_id  # ä¿å­˜å¼€å§‹æ ‡è®°ID
        self.eos_token_id = eos_token_id  # ä¿å­˜ç»“æŸæ ‡è®°ID
        self.hidden_act = hidden_act  # ä¿å­˜æ¿€æ´»å‡½æ•°ç±»å‹
        self.hidden_size = hidden_size  # ä¿å­˜éšè—å±‚ç»´åº¦
        self.intermediate_size = intermediate_size  # ä¿å­˜ä¸­é—´å±‚å¤§å°
        self.max_position_embeddings = max_position_embeddings  # ä¿å­˜æœ€å¤§ä½ç½®ç¼–ç æ•°
        self.num_attention_heads = num_attention_heads  # ä¿å­˜æ³¨æ„åŠ›å¤´æ•°é‡
        self.num_hidden_layers = num_hidden_layers  # ä¿å­˜å±‚æ•°
        self.num_key_value_heads = num_key_value_heads  # ä¿å­˜KVå¤´æ•°é‡
        self.vocab_size = vocab_size  # ä¿å­˜è¯æ±‡è¡¨å¤§å°
        self.rms_norm_eps = rms_norm_eps  # ä¿å­˜å½’ä¸€åŒ–epsilon
        self.rope_theta = rope_theta  # ä¿å­˜RoPEå‚æ•°
        self.flash_attn = flash_attn  # ä¿å­˜æ˜¯å¦ä½¿ç”¨Flash Attention
        ####################################################
        # ä¿å­˜æ··åˆä¸“å®¶æ¨¡å‹çš„é…ç½®å‚æ•°
        ####################################################
        self.use_moe = use_moe  # æ˜¯å¦ä½¿ç”¨MoE
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶æ•°é‡
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ç±»å‹
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±æƒé‡
        self.seq_aux = seq_aux  # æ˜¯å¦ä½¿ç”¨åºåˆ—çº§è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦å½’ä¸€åŒ–top-kæ¦‚ç‡


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Model
#                                             MiniMind æ¨¡å‹å®ç°
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

# å¯¼å…¥å¿…è¦çš„åº“
import math  # æ•°å­¦è¿ç®—åº“
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import nn  # PyTorchçš„ç¥ç»ç½‘ç»œæ¨¡å—
from transformers.activations import ACT2FN  # transformersåº“ä¸­çš„æ¿€æ´»å‡½æ•°å­—å…¸
from typing import Optional, Tuple, List, Union  # ç±»å‹æç¤º
import torch.nn.functional as F  # PyTorchçš„å‡½æ•°å¼API
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig  # transformersçš„åŸºç±»
from transformers.modeling_outputs import CausalLMOutputWithPast  # æ¨¡å‹è¾“å‡ºæ ¼å¼


# RMSNormå½’ä¸€åŒ–å±‚
# è¿™æ˜¯ä¸€ç§æ¯”LayerNormæ›´ç®€å•é«˜æ•ˆçš„å½’ä¸€åŒ–æ–¹æ³•ï¼Œå¸¸ç”¨äºå¤§è¯­è¨€æ¨¡å‹
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        """
        åˆå§‹åŒ–RMSNormå±‚
        å‚æ•°:
            dim: è¾“å…¥ç‰¹å¾çš„ç»´åº¦
            eps: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
        """
        super().__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        self.eps = eps  # ä¿å­˜epsilonå€¼
        # åˆ›å»ºå¯å­¦ä¹ çš„æƒé‡å‚æ•°ï¼Œåˆå§‹åŒ–ä¸ºå…¨1
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        """
        æ‰§è¡ŒRMSå½’ä¸€åŒ–çš„æ ¸å¿ƒè®¡ç®—
        RMS = Root Mean Squareï¼ˆå‡æ–¹æ ¹ï¼‰
        å…¬å¼: x / sqrt(mean(x^2) + eps)
        """
        # x.pow(2): å¯¹xçš„æ¯ä¸ªå…ƒç´ å¹³æ–¹
        # .mean(-1, keepdim=True): åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šæ±‚å¹³å‡ï¼Œä¿æŒç»´åº¦
        # + self.eps: åŠ ä¸Šepsiloné˜²æ­¢é™¤é›¶
        # torch.rsqrt(): è®¡ç®—å¹³æ–¹æ ¹çš„å€’æ•°ï¼Œå³ 1/sqrt(x)
        # x * ...: å°†åŸå§‹å€¼ä¹˜ä»¥å½’ä¸€åŒ–å› å­
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
        è¿”å›:
            å½’ä¸€åŒ–å¹¶ç¼©æ”¾åçš„å¼ é‡
        """
        # å…ˆå°†xè½¬ä¸ºfloatç±»å‹è¿›è¡Œå½’ä¸€åŒ–ï¼Œç„¶åä¹˜ä»¥å¯å­¦ä¹ çš„æƒé‡
        # type_as(x)å°†ç»“æœè½¬å›xçš„åŸå§‹æ•°æ®ç±»å‹
        return self.weight * self._norm(x.float()).type_as(x)


def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    """
    é¢„è®¡ç®—RoPE(Rotary Position Embedding)æ—‹è½¬ä½ç½®ç¼–ç çš„é¢‘ç‡
    RoPEæ˜¯ä¸€ç§å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ–¹æ³•
    
    å‚æ•°:
        dim: æ³¨æ„åŠ›å¤´çš„ç»´åº¦
        end: æœ€å¤§åºåˆ—é•¿åº¦
        theta: åŸºç¡€é¢‘ç‡å‚æ•°ï¼Œæ§åˆ¶ä½ç½®ç¼–ç çš„å‘¨æœŸ
    è¿”å›:
        freqs_cos: é¢„è®¡ç®—çš„ä½™å¼¦å€¼
        freqs_sin: é¢„è®¡ç®—çš„æ­£å¼¦å€¼
    """
    # è®¡ç®—é¢‘ç‡åºåˆ—
    # torch.arange(0, dim, 2): ç”Ÿæˆ[0, 2, 4, ..., dim-2]
    # [: (dim // 2)]: å–å‰dim//2ä¸ªå…ƒç´ 
    # / dim: å½’ä¸€åŒ–åˆ°[0, 1)èŒƒå›´
    # theta ** (...): è®¡ç®—thetaçš„å¹‚æ¬¡
    # 1.0 / (...): å–å€’æ•°å¾—åˆ°é¢‘ç‡
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    
    # ç”Ÿæˆä½ç½®ç´¢å¼•åºåˆ— [0, 1, 2, ..., end-1]
    t = torch.arange(end, device=freqs.device)
    
    # torch.outerè®¡ç®—å¤–ç§¯ï¼Œç”Ÿæˆä½ç½®å’Œé¢‘ç‡çš„æ‰€æœ‰ç»„åˆ
    # ç»“æœå½¢çŠ¶: [end, dim//2]
    freqs = torch.outer(t, freqs).float()
    
    # è®¡ç®—ä½™å¼¦å€¼å¹¶æ‹¼æ¥ä¸¤æ¬¡ï¼ˆç”¨äºåç»­çš„æ—‹è½¬æ“ä½œï¼‰
    # torch.catåœ¨æœ€åä¸€ä¸ªç»´åº¦æ‹¼æ¥ï¼Œå½¢çŠ¶å˜ä¸º: [end, dim]
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    
    # è®¡ç®—æ­£å¼¦å€¼å¹¶æ‹¼æ¥ä¸¤æ¬¡
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    
    # è¿”å›é¢„è®¡ç®—çš„ä½™å¼¦å’Œæ­£å¼¦å€¼
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """
    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ°æŸ¥è¯¢(Q)å’Œé”®(K)å‘é‡
    è¿™æ˜¯RoPEçš„æ ¸å¿ƒæ“ä½œï¼Œé€šè¿‡æ—‹è½¬å‘é‡æ¥ç¼–ç ä½ç½®ä¿¡æ¯
    
    å‚æ•°:
        q: æŸ¥è¯¢å‘é‡
        k: é”®å‘é‡
        cos: é¢„è®¡ç®—çš„ä½™å¼¦å€¼
        sin: é¢„è®¡ç®—çš„æ­£å¼¦å€¼
        position_ids: ä½ç½®IDï¼ˆå¯é€‰ï¼‰
        unsqueeze_dim: éœ€è¦æ‰©å±•ç»´åº¦çš„ä½ç½®
    è¿”å›:
        q_embed: åº”ç”¨ä½ç½®ç¼–ç åçš„æŸ¥è¯¢å‘é‡
        k_embed: åº”ç”¨ä½ç½®ç¼–ç åçš„é”®å‘é‡
    """
    def rotate_half(x):
        """
        å°†å‘é‡çš„ååŠéƒ¨åˆ†ç§»åˆ°å‰é¢ï¼Œå‰åŠéƒ¨åˆ†ç§»åˆ°åé¢ï¼Œå¹¶å¯¹å‰åŠéƒ¨åˆ†å–è´Ÿ
        è¿™æ˜¯å®ç°æ—‹è½¬çš„å…³é”®æ­¥éª¤
        ä¾‹å¦‚: [a, b, c, d] -> [-c, -d, a, b]
        """
        # x.shape[-1] // 2: è·å–å‘é‡é•¿åº¦çš„ä¸€åŠ
        # x[..., x.shape[-1] // 2:]: å–ååŠéƒ¨åˆ†
        # x[..., : x.shape[-1] // 2]: å–å‰åŠéƒ¨åˆ†
        # -x[..., x.shape[-1] // 2:]: å¯¹ååŠéƒ¨åˆ†å–è´Ÿ
        # torch.catæ‹¼æ¥: å°†å–è´Ÿçš„ååŠéƒ¨åˆ†å’Œå‰åŠéƒ¨åˆ†æ‹¼æ¥
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    # å¯¹æŸ¥è¯¢å‘é‡åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
    # å…¬å¼: q_embed = q * cos + rotate_half(q) * sin
    # unsqueeze(unsqueeze_dim)åœ¨æŒ‡å®šç»´åº¦å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥ä¾¿å¹¿æ’­
    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    
    # å¯¹é”®å‘é‡åº”ç”¨ç›¸åŒçš„æ—‹è½¬ä½ç½®ç¼–ç 
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    
    # è¿”å›ç¼–ç åçš„æŸ¥è¯¢å’Œé”®å‘é‡
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    é‡å¤é”®å€¼(KV)å¼ é‡ä»¥åŒ¹é…æŸ¥è¯¢(Q)çš„å¤´æ•°
    è¿™ç”¨äºå®ç°åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(Grouped Query Attention, GQA)
    GQAå¯ä»¥å‡å°‘KVç¼“å­˜çš„å†…å­˜å ç”¨ï¼Œæé«˜æ¨ç†æ•ˆç‡
    
    å‚æ•°:
        x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, num_kv_heads, head_dim]
        n_rep: é‡å¤æ¬¡æ•°ï¼Œå³æ¯ä¸ªKVå¤´éœ€è¦æœåŠ¡å¤šå°‘ä¸ªQå¤´
    è¿”å›:
        é‡å¤åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, num_kv_heads * n_rep, head_dim]
    
    ç­‰ä»·äº: torch.repeat_interleave(x, dim=2, repeats=n_rep)
    """
    # è·å–è¾“å…¥å¼ é‡çš„å„ä¸ªç»´åº¦
    bs, slen, num_key_value_heads, head_dim = x.shape
    
    # å¦‚æœä¸éœ€è¦é‡å¤ï¼ˆn_rep=1ï¼‰ï¼Œç›´æ¥è¿”å›åŸå¼ é‡
    if n_rep == 1:
        return x
    
    # é‡å¤KVå¼ é‡çš„æ­¥éª¤:
    # 1. x[:, :, :, None, :]: åœ¨ç¬¬4ä¸ªç»´åº¦æ’å…¥ä¸€ä¸ªæ–°ç»´åº¦
    #    å½¢çŠ¶å˜ä¸º: [bs, slen, num_kv_heads, 1, head_dim]
    # 2. .expand(...): åœ¨æ–°ç»´åº¦ä¸Šæ‰©å±•n_repæ¬¡ï¼ˆä¸å¤åˆ¶æ•°æ®ï¼Œåªæ˜¯æ”¹å˜è§†å›¾ï¼‰
    #    å½¢çŠ¶å˜ä¸º: [bs, slen, num_kv_heads, n_rep, head_dim]
    # 3. .reshape(...): å°†num_kv_headså’Œn_repä¸¤ä¸ªç»´åº¦åˆå¹¶
    #    æœ€ç»ˆå½¢çŠ¶: [bs, slen, num_kv_heads * n_rep, head_dim]
    return (
        x[:, :, :, None, :]
        .expand(bs, slen, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )


class Attention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶(Multi-Head Attention)
    è¿™æ˜¯Transformerçš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºè®©æ¨¡å‹å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒä½ç½®
    """
    def __init__(self, args: MiniMindConfig):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›å±‚
        å‚æ•°:
            args: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        
        # ç¡®å®šKVå¤´çš„æ•°é‡ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨ä¸Qå¤´ç›¸åŒçš„æ•°é‡
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        
        # ç¡®ä¿Qå¤´æ•°é‡èƒ½è¢«KVå¤´æ•°é‡æ•´é™¤ï¼ˆç”¨äºåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰
        assert args.num_attention_heads % self.num_key_value_heads == 0
        
        # ä¿å­˜æ³¨æ„åŠ›å¤´çš„æ•°é‡
        self.n_local_heads = args.num_attention_heads  # Qå¤´çš„æ•°é‡
        self.n_local_kv_heads = self.num_key_value_heads  # KVå¤´çš„æ•°é‡
        
        # è®¡ç®—æ¯ä¸ªKVå¤´éœ€è¦æœåŠ¡å¤šå°‘ä¸ªQå¤´
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        
        # è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
        self.head_dim = args.hidden_size // args.num_attention_heads
        
        # å®šä¹‰Qã€Kã€Vçš„çº¿æ€§æŠ•å½±å±‚ï¼ˆä¸ä½¿ç”¨åç½®ä»¥å‡å°‘å‚æ•°ï¼‰
        # QæŠ•å½±: å°†hidden_sizeç»´åº¦æ˜ å°„åˆ° num_heads * head_dim
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        # KæŠ•å½±: ä½¿ç”¨è¾ƒå°‘çš„KVå¤´ä»¥èŠ‚çœå†…å­˜
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        # VæŠ•å½±: ä¸Kä½¿ç”¨ç›¸åŒæ•°é‡çš„å¤´
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        
        # è¾“å‡ºæŠ•å½±å±‚: å°†å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºæ˜ å°„å›hidden_sizeç»´åº¦
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        
        # Dropoutå±‚ç”¨äºæ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        self.attn_dropout = nn.Dropout(args.dropout)  # æ³¨æ„åŠ›æƒé‡çš„dropout
        self.resid_dropout = nn.Dropout(args.dropout)  # æ®‹å·®è¿æ¥çš„dropout
        self.dropout = args.dropout  # ä¿å­˜dropoutæ¯”ä¾‹
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨Flash Attentionï¼ˆPyTorch 2.0+çš„ä¼˜åŒ–å®ç°ï¼‰
        # Flash Attentionå¯ä»¥æ˜¾è‘—åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—å¹¶å‡å°‘å†…å­˜ä½¿ç”¨
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
        # å¦‚æœä¸æ”¯æŒFlash Attentionï¼Œå¯ä»¥æ‰“å°è­¦å‘Šï¼ˆå·²æ³¨é‡Šï¼‰
        # print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # æ¥æ”¶é¢„è®¡ç®—çš„coså’Œsin
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # KVç¼“å­˜
                use_cache=False,  # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
                attention_mask: Optional[torch.Tensor] = None):  # æ³¨æ„åŠ›æ©ç 
        """
        æ³¨æ„åŠ›å±‚çš„å‰å‘ä¼ æ’­
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, hidden_size]
            position_embeddings: ä½ç½®ç¼–ç çš„(cos, sin)å…ƒç»„
            past_key_value: ä¹‹å‰ç¼“å­˜çš„(K, V)ï¼Œç”¨äºåŠ é€Ÿç”Ÿæˆ
            use_cache: æ˜¯å¦è¿”å›å½“å‰çš„KVç¼“å­˜
            attention_mask: æ³¨æ„åŠ›æ©ç ï¼Œç”¨äºå±è”½æŸäº›ä½ç½®
        è¿”å›:
            output: æ³¨æ„åŠ›è¾“å‡º
            past_kv: å½“å‰çš„KVç¼“å­˜ï¼ˆå¦‚æœuse_cache=Trueï¼‰
        """
        # è·å–è¾“å…¥çš„å½¢çŠ¶: batch_size, åºåˆ—é•¿åº¦, éšè—ç»´åº¦
        bsz, seq_len, _ = x.shape
        
        # é€šè¿‡çº¿æ€§å±‚è®¡ç®—Qã€Kã€V
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        
        # é‡å¡‘Qã€Kã€Vçš„å½¢çŠ¶ä»¥åˆ†ç¦»å‡ºå¤šä¸ªæ³¨æ„åŠ›å¤´
        # ä» [bsz, seq_len, total_dim] å˜ä¸º [bsz, seq_len, num_heads, head_dim]
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)

        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç (RoPE)åˆ°Qå’ŒK
        cos, sin = position_embeddings  # è§£åŒ…ä½ç½®ç¼–ç 
        # åªä½¿ç”¨å½“å‰åºåˆ—é•¿åº¦å¯¹åº”çš„ä½ç½®ç¼–ç 
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # KVç¼“å­˜å®ç°ï¼ˆç”¨äºåŠ é€Ÿè‡ªå›å½’ç”Ÿæˆï¼‰
        # å¦‚æœæœ‰ä¹‹å‰çš„ç¼“å­˜ï¼Œå°†æ–°çš„Kã€Væ‹¼æ¥åˆ°ç¼“å­˜åé¢
        if past_key_value is not None:
            # åœ¨åºåˆ—ç»´åº¦(dim=1)ä¸Šæ‹¼æ¥
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        
        # å¦‚æœéœ€è¦ç¼“å­˜ï¼Œä¿å­˜å½“å‰çš„Kã€Vï¼›å¦åˆ™è¿”å›None
        past_kv = (xk, xv) if use_cache else None

        # è°ƒæ•´å¼ é‡å½¢çŠ¶ä»¥è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
        # transpose(1, 2): å°†seq_lenå’Œnum_headsç»´åº¦äº¤æ¢
        # ä» [bsz, seq_len, num_heads, head_dim] å˜ä¸º [bsz, num_heads, seq_len, head_dim]
        xq, xk, xv = (
            xq.transpose(1, 2),  # Q: [bsz, n_heads, seq_len, head_dim]
            repeat_kv(xk, self.n_rep).transpose(1, 2),  # K: é‡å¤KVå¤´ä»¥åŒ¹é…Qå¤´æ•°é‡
            repeat_kv(xv, self.n_rep).transpose(1, 2)   # V: åŒä¸Š
        )

        # æ ¹æ®æ¡ä»¶é€‰æ‹©ä½¿ç”¨Flash Attentionè¿˜æ˜¯æ ‡å‡†æ³¨æ„åŠ›
        if self.flash and seq_len != 1:
            # ä½¿ç”¨Flash Attentionï¼ˆæ›´å¿«æ›´çœå†…å­˜ï¼‰
            # è®­ç»ƒæ—¶ä½¿ç”¨dropoutï¼Œæ¨ç†æ—¶ä¸ä½¿ç”¨
            dropout_p = self.dropout if self.training else 0.0
            attn_mask = None
            
            # å¦‚æœæä¾›äº†attention_maskï¼Œéœ€è¦è°ƒæ•´å…¶å½¢çŠ¶
            if attention_mask is not None:
                # æ‰©å±•maskçš„ç»´åº¦ä»¥åŒ¹é…æ³¨æ„åŠ›çŸ©é˜µçš„å½¢çŠ¶
                attn_mask = attention_mask.view(bsz, 1, 1, -1).expand(bsz, self.n_local_heads, seq_len, -1)
                attn_mask = attn_mask.bool() if attention_mask is not None else None

            # è°ƒç”¨PyTorchçš„ä¼˜åŒ–æ³¨æ„åŠ›å‡½æ•°
            # is_causal=Trueè¡¨ç¤ºä½¿ç”¨å› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰ï¼Œç”¨äºè‡ªå›å½’ç”Ÿæˆ
            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)
        else:
            # ä½¿ç”¨æ ‡å‡†çš„æ³¨æ„åŠ›è®¡ç®—ï¼ˆæ‰‹åŠ¨å®ç°ï¼‰
            # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: Q @ K^T / sqrt(head_dim)
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            
            # 2. æ·»åŠ å› æœæ©ç ï¼ˆä¸Šä¸‰è§’è®¾ä¸ºè´Ÿæ— ç©·ï¼Œsoftmaxåå˜ä¸º0ï¼‰
            # torch.triuåˆ›å»ºä¸Šä¸‰è§’çŸ©é˜µï¼Œdiagonal=1è¡¨ç¤ºä¸»å¯¹è§’çº¿ä¸Šæ–¹
            # è¿™ç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å®ƒä¹‹å‰çš„ä½ç½®ï¼ˆè‡ªå›å½’ç‰¹æ€§ï¼‰
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)  # å¢åŠ batchå’Œheadç»´åº¦

            # 3. å¦‚æœæœ‰é¢å¤–çš„attention_maskï¼Œä¹Ÿæ·»åŠ è¿›å»
            if attention_mask is not None:
                # æ‰©å±•maskç»´åº¦
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                # å°†0/1 maskè½¬æ¢ä¸ºåŠ æ³•maskï¼ˆ0å˜ä¸º0ï¼Œ1å˜ä¸º-1e9ï¼‰
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask

            # 4. åº”ç”¨softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            
            # 5. åº”ç”¨dropout
            scores = self.attn_dropout(scores)
            
            # 6. ç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒV: attention_weights @ V
            output = scores @ xv

        # è°ƒæ•´è¾“å‡ºå½¢çŠ¶
        # transpose(1, 2): [bsz, n_heads, seq_len, head_dim] -> [bsz, seq_len, n_heads, head_dim]
        # reshape: åˆå¹¶æ‰€æœ‰å¤´çš„è¾“å‡º -> [bsz, seq_len, n_heads * head_dim]
        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        
        # é€šè¿‡è¾“å‡ºæŠ•å½±å±‚å’Œdropout
        output = self.resid_dropout(self.o_proj(output))
        
        # è¿”å›è¾“å‡ºå’ŒKVç¼“å­˜
        return output, past_kv


class FeedForward(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œ(Feed-Forward Network, FFN)
    è¿™æ˜¯Transformerä¸­çš„å¦ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åœ°è¿›è¡Œéçº¿æ€§å˜æ¢
    ä½¿ç”¨SwiGLUæ¿€æ´»å‡½æ•°ï¼ˆSwish-Gated Linear Unitï¼‰
    """
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–å‰é¦ˆç½‘ç»œ
        å‚æ•°:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        
        # å¦‚æœæœªæŒ‡å®šä¸­é—´å±‚å¤§å°ï¼Œè‡ªåŠ¨è®¡ç®—
        if config.intermediate_size is None:
            # é€šå¸¸è®¾ç½®ä¸ºhidden_sizeçš„8/3å€ï¼ˆçº¦2.67å€ï¼‰
            intermediate_size = int(config.hidden_size * 8 / 3)
            # å‘ä¸Šå–æ•´åˆ°64çš„å€æ•°ï¼ˆæœ‰åˆ©äºç¡¬ä»¶åŠ é€Ÿï¼‰
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        
        # å®šä¹‰ä¸‰ä¸ªçº¿æ€§æŠ•å½±å±‚ï¼ˆSwiGLUéœ€è¦ä¸‰ä¸ªæŠ•å½±ï¼‰
        # gate_proj: é—¨æ§æŠ•å½±ï¼Œç”¨äºæ¿€æ´»å‡½æ•°
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        # down_proj: ä¸‹æŠ•å½±ï¼Œå°†ä¸­é—´ç»´åº¦æ˜ å°„å›hidden_size
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        # up_proj: ä¸ŠæŠ•å½±ï¼Œä¸gate_projé…åˆå®ç°é—¨æ§æœºåˆ¶
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        
        # Dropoutå±‚ç”¨äºæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(config.dropout)
        
        # æ¿€æ´»å‡½æ•°ï¼ˆä»é…ç½®ä¸­è·å–ï¼Œé€šå¸¸æ˜¯'silu'ï¼‰
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        """
        å‰é¦ˆç½‘ç»œçš„å‰å‘ä¼ æ’­
        å®ç°SwiGLU: FFN(x) = (Swish(gate_proj(x)) * up_proj(x)) @ down_proj
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
        è¿”å›:
            ç»è¿‡å‰é¦ˆç½‘ç»œå¤„ç†åçš„å¼ é‡
        """
        # SwiGLUçš„è®¡ç®—æ­¥éª¤:
        # 1. gate_proj(x): é€šè¿‡é—¨æ§æŠ•å½±
        # 2. act_fn(...): åº”ç”¨æ¿€æ´»å‡½æ•°ï¼ˆå¦‚SiLU/Swishï¼‰
        # 3. up_proj(x): é€šè¿‡ä¸ŠæŠ•å½±
        # 4. ä¸¤è€…é€å…ƒç´ ç›¸ä¹˜ï¼ˆé—¨æ§æœºåˆ¶ï¼‰
        # 5. down_proj(...): æŠ•å½±å›åŸå§‹ç»´åº¦
        # 6. dropout(...): åº”ç”¨dropoutæ­£åˆ™åŒ–
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


class MoEGate(nn.Module):
    """
    æ··åˆä¸“å®¶æ¨¡å‹çš„é—¨æ§ç½‘ç»œ(Mixture of Experts Gate)
    è´Ÿè´£ä¸ºæ¯ä¸ªtokené€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶è¿›è¡Œå¤„ç†
    """
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–MoEé—¨æ§ç½‘ç»œ
        å‚æ•°:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        self.config = config
        
        # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡ï¼ˆtop-kï¼‰
        self.top_k = config.num_experts_per_tok
        # å¯è·¯ç”±çš„ä¸“å®¶æ€»æ•°
        self.n_routed_experts = config.n_routed_experts

        # è¯„åˆ†å‡½æ•°ç±»å‹ï¼ˆç”¨äºè®¡ç®—ä¸“å®¶é€‰æ‹©æ¦‚ç‡ï¼‰
        self.scoring_func = config.scoring_func
        # è¾…åŠ©æŸå¤±çš„æƒé‡ç³»æ•°ï¼ˆç”¨äºå¹³è¡¡ä¸“å®¶è´Ÿè½½ï¼‰
        self.alpha = config.aux_loss_alpha
        # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤±
        self.seq_aux = config.seq_aux

        # æ˜¯å¦å½’ä¸€åŒ–top-kæ¦‚ç‡
        self.norm_topk_prob = config.norm_topk_prob
        # é—¨æ§ç½‘ç»œçš„è¾“å…¥ç»´åº¦
        self.gating_dim = config.hidden_size
        
        # é—¨æ§æƒé‡çŸ©é˜µ: [num_experts, hidden_size]
        # ç”¨äºè®¡ç®—æ¯ä¸ªtokenå¯¹æ¯ä¸ªä¸“å®¶çš„äº²å’Œåº¦åˆ†æ•°
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        # åˆå§‹åŒ–æƒé‡
        self.reset_parameters()

    def reset_parameters(self) -> None:
        """
        ä½¿ç”¨Kaimingå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡
        è¿™ç§åˆå§‹åŒ–æ–¹æ³•æœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§
        """
        import torch.nn.init as init
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        """
        é—¨æ§ç½‘ç»œçš„å‰å‘ä¼ æ’­
        ä¸ºæ¯ä¸ªtokené€‰æ‹©top-kä¸ªä¸“å®¶åŠå…¶æƒé‡
        
        å‚æ•°:
            hidden_states: è¾“å…¥éšè—çŠ¶æ€ [batch_size, seq_len, hidden_size]
        è¿”å›:
            topk_idx: é€‰ä¸­çš„ä¸“å®¶ç´¢å¼• [batch_size * seq_len, top_k]
            topk_weight: å¯¹åº”çš„ä¸“å®¶æƒé‡ [batch_size * seq_len, top_k]
            aux_loss: è¾…åŠ©æŸå¤±ï¼ˆç”¨äºå¹³è¡¡ä¸“å®¶è´Ÿè½½ï¼‰
        """
        # è·å–è¾“å…¥å½¢çŠ¶
        bsz, seq_len, h = hidden_states.shape
        
        # å°†è¾“å…¥å±•å¹³ä¸º2D: [batch_size * seq_len, hidden_size]
        hidden_states = hidden_states.view(-1, h)
        
        # è®¡ç®—æ¯ä¸ªtokenå¯¹æ¯ä¸ªä¸“å®¶çš„logitsï¼ˆæœªå½’ä¸€åŒ–çš„åˆ†æ•°ï¼‰
        # logitså½¢çŠ¶: [batch_size * seq_len, num_experts]
        logits = F.linear(hidden_states, self.weight, None)
        
        # æ ¹æ®è¯„åˆ†å‡½æ•°è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        if self.scoring_func == 'softmax':
            # ä½¿ç”¨softmaxå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
            scores = logits.softmax(dim=-1)
        else:
            # å¦‚æœä½¿ç”¨äº†ä¸æ”¯æŒçš„è¯„åˆ†å‡½æ•°ï¼ŒæŠ›å‡ºé”™è¯¯
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        # é€‰æ‹©top-kä¸ªä¸“å®¶
        # topk_weight: æœ€é«˜çš„kä¸ªæ¦‚ç‡å€¼
        # topk_idx: å¯¹åº”çš„ä¸“å®¶ç´¢å¼•
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        # å¦‚æœéœ€è¦ï¼Œå¯¹top-kæƒé‡è¿›è¡Œå½’ä¸€åŒ–
        # è¿™ç¡®ä¿é€‰ä¸­çš„ä¸“å®¶æƒé‡å’Œä¸º1
        if self.top_k > 1 and self.norm_topk_prob:
            # è®¡ç®—æƒé‡å’Œï¼ˆåŠ ä¸Šå°å¸¸æ•°é˜²æ­¢é™¤é›¶ï¼‰
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            # å½’ä¸€åŒ–
            topk_weight = topk_weight / denominator

        # è®¡ç®—è¾…åŠ©æŸå¤±ï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼‰
        # è¾…åŠ©æŸå¤±ç”¨äºé¼“åŠ±ä¸“å®¶è´Ÿè½½å‡è¡¡ï¼Œé˜²æ­¢æŸäº›ä¸“å®¶è¢«è¿‡åº¦ä½¿ç”¨
        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            # é‡å¡‘ç´¢å¼•ä»¥ä¾¿è®¡ç®—æŸå¤±
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1)
            
            if self.seq_aux:
                # åºåˆ—çº§è¾…åŠ©æŸå¤±
                # é‡å¡‘åˆ†æ•°: [batch_size, seq_len, num_experts]
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
                
                # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                # scatter_add_ç´¯åŠ æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                
                # è¾…åŠ©æŸå¤± = ä¸“å®¶ä½¿ç”¨é¢‘ç‡ * ä¸“å®¶å¹³å‡åˆ†æ•°
                # è¿™é¼“åŠ±é«˜åˆ†æ•°çš„ä¸“å®¶è¢«æ›´å¤šä½¿ç”¨ï¼ŒåŒæ—¶å¹³è¡¡è´Ÿè½½
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                # Tokençº§è¾…åŠ©æŸå¤±
                # åˆ›å»ºone-hotç¼–ç 
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¯”ä¾‹
                ce = mask_ce.float().mean(0)
                # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡åˆ†æ•°
                Pi = scores_for_aux.mean(0)
                # ä¸“å®¶è´Ÿè½½å› å­
                fi = ce * self.n_routed_experts
                # è¾…åŠ©æŸå¤±: é¼“åŠ±è´Ÿè½½å‡è¡¡
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            # æ¨ç†æ—¶ä¸è®¡ç®—è¾…åŠ©æŸå¤±
            aux_loss = 0
        
        # è¿”å›é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•ã€æƒé‡å’Œè¾…åŠ©æŸå¤±
        return topk_idx, topk_weight, aux_loss


class MOEFeedForward(nn.Module):
    """
    æ··åˆä¸“å®¶å‰é¦ˆç½‘ç»œ(Mixture of Experts Feed-Forward Network)
    ä½¿ç”¨å¤šä¸ªä¸“å®¶ç½‘ç»œå’Œé—¨æ§æœºåˆ¶ï¼Œæ¯ä¸ªtokenç”±é€‰å®šçš„ä¸“å®¶å¤„ç†
    è¿™å¯ä»¥å¢åŠ æ¨¡å‹å®¹é‡è€Œä¸æˆæ¯”ä¾‹åœ°å¢åŠ è®¡ç®—é‡
    """
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–MoEå‰é¦ˆç½‘ç»œ
        å‚æ•°:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        self.config = config
        
        # åˆ›å»ºå¤šä¸ªä¸“å®¶ç½‘ç»œï¼ˆæ¯ä¸ªéƒ½æ˜¯ä¸€ä¸ªFeedForwardç½‘ç»œï¼‰
        # è¿™äº›ä¸“å®¶ä¼šæ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)  # åˆ›å»ºn_routed_expertsä¸ªä¸“å®¶
        ])
        
        # é—¨æ§ç½‘ç»œï¼Œç”¨äºé€‰æ‹©ä¸“å®¶
        self.gate = MoEGate(config)
        
        # å¦‚æœé…ç½®äº†å…±äº«ä¸“å®¶ï¼Œåˆ›å»ºå®ƒä»¬
        # å…±äº«ä¸“å®¶ä¼šå¤„ç†æ‰€æœ‰tokenï¼Œä¸ç»è¿‡é—¨æ§é€‰æ‹©
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])

    def forward(self, x):
        """
        MoEå‰é¦ˆç½‘ç»œçš„å‰å‘ä¼ æ’­
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, hidden_size]
        è¿”å›:
            å¤„ç†åçš„è¾“å‡ºå¼ é‡
        """
        # ä¿å­˜è¾“å…¥çš„å‰¯æœ¬ï¼ˆç”¨äºå…±äº«ä¸“å®¶å’Œæ®‹å·®è¿æ¥ï¼‰
        identity = x
        # ä¿å­˜åŸå§‹å½¢çŠ¶
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        
        # ä½¿ç”¨é—¨æ§æœºåˆ¶é€‰æ‹©ä¸“å®¶
        # topk_idx: æ¯ä¸ªtokené€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
        # topk_weight: å¯¹åº”çš„æƒé‡
        # aux_loss: è¾…åŠ©æŸå¤±
        topk_idx, topk_weight, aux_loss = self.gate(x)
        
        # å°†è¾“å…¥å±•å¹³ä¸º2D: [batch_size * seq_len, hidden_size]
        x = x.view(-1, x.shape[-1])
        # å°†ä¸“å®¶ç´¢å¼•ä¹Ÿå±•å¹³
        flat_topk_idx = topk_idx.view(-1)
        
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨ç®€å•ä½†å†…å­˜æ•ˆç‡è¾ƒä½çš„æ–¹æ³•
            # å°†æ¯ä¸ªtokené‡å¤num_experts_per_tokæ¬¡ï¼ˆå› ä¸ºæ¯ä¸ªtokenä¼šè¢«å¤šä¸ªä¸“å®¶å¤„ç†ï¼‰
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
            
            # åˆ›å»ºè¾“å‡ºå¼ é‡
            y = torch.empty_like(x, dtype=torch.float16)
            
            # éå†æ‰€æœ‰ä¸“å®¶ï¼Œè®©æ¯ä¸ªä¸“å®¶å¤„ç†åˆ†é…ç»™å®ƒçš„token
            for i, expert in enumerate(self.experts):
                # æ‰¾å‡ºåˆ†é…ç»™ä¸“å®¶içš„æ‰€æœ‰token
                # è®©ä¸“å®¶iå¤„ç†è¿™äº›token
                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to(y.dtype)  # ç¡®ä¿ç±»å‹ä¸€è‡´
            
            # é‡å¡‘è¾“å‡ºå¹¶åº”ç”¨ä¸“å®¶æƒé‡
            # å°†è¾“å‡ºreshapeä¸º [batch_size * seq_len, num_experts_per_tok, hidden_size]
            # ä¹˜ä»¥æƒé‡ååœ¨ä¸“å®¶ç»´åº¦ä¸Šæ±‚å’Œ
            y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
            
            # æ¢å¤åŸå§‹å½¢çŠ¶
            y = y.view(*orig_shape)
        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)
        
        # å¦‚æœæœ‰å…±äº«ä¸“å®¶ï¼Œå°†å®ƒä»¬çš„è¾“å‡ºåŠ åˆ°ç»“æœä¸Š
        # å…±äº«ä¸“å®¶å¤„ç†æ‰€æœ‰tokenï¼Œä¸ç»è¿‡é—¨æ§é€‰æ‹©
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity)
        
        # ä¿å­˜è¾…åŠ©æŸå¤±ï¼ˆç”¨äºè®­ç»ƒæ—¶çš„æŸå¤±è®¡ç®—ï¼‰
        self.aux_loss = aux_loss
        return y

    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        """
        MoEæ¨ç†çš„ä¼˜åŒ–å®ç°
        é€šè¿‡æ‰¹é‡å¤„ç†æ¯ä¸ªä¸“å®¶çš„æ‰€æœ‰tokenæ¥æé«˜æ•ˆç‡
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ [num_tokens, hidden_size]
            flat_expert_indices: ä¸“å®¶ç´¢å¼• [num_tokens * num_experts_per_tok]
            flat_expert_weights: ä¸“å®¶æƒé‡ [num_tokens * num_experts_per_tok, 1]
        è¿”å›:
            å¤„ç†åçš„è¾“å‡ºå¼ é‡
        """
        # åˆ›å»ºè¾“å‡ºç¼“å­˜ï¼Œåˆå§‹åŒ–ä¸ºé›¶
        expert_cache = torch.zeros_like(x)
        
        # å¯¹ä¸“å®¶ç´¢å¼•æ’åºï¼Œè¿™æ ·ç›¸åŒä¸“å®¶çš„tokenä¼šèšåœ¨ä¸€èµ·
        idxs = flat_expert_indices.argsort()
        
        # è®¡ç®—æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°é‡çš„ç´¯ç§¯å’Œ
        # ä¾‹å¦‚: [6, 15, 20, 26] è¡¨ç¤ºä¸“å®¶0å¤„ç†6ä¸ªtokenï¼Œä¸“å®¶1å¤„ç†9ä¸ªtokenç­‰
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        
        # è®¡ç®—åŸå§‹tokenç´¢å¼•
        token_idxs = idxs // self.config.num_experts_per_tok
        
        # ç¤ºä¾‹è¯´æ˜:
        # å½“tokens_per_expert = [6, 15, 20, 26]æ—¶ï¼Œæœ‰4ä¸ªä¸“å®¶
        # token_idxs = [3, 7, 19, 21, 24, 25, 4, 5, 6, 10, 11, 12...]
        # token_idxs[:6] -> [3, 7, 19, 21, 24, 25] æ˜¯ä¸“å®¶0å¤„ç†çš„tokenä½ç½®
        # token_idxs[6:15] -> [4, 5, 6, 10, 11, 12...] æ˜¯ä¸“å®¶1å¤„ç†çš„tokenä½ç½®
        # ä¾æ­¤ç±»æ¨...
        
        # éå†æ¯ä¸ªä¸“å®¶
        for i, end_idx in enumerate(tokens_per_expert):
            # è®¡ç®—å½“å‰ä¸“å®¶å¤„ç†çš„tokenèŒƒå›´
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]
            
            # å¦‚æœè¿™ä¸ªä¸“å®¶æ²¡æœ‰åˆ†é…åˆ°tokenï¼Œè·³è¿‡
            if start_idx == end_idx:
                continue
            
            # è·å–å½“å‰ä¸“å®¶
            expert = self.experts[i]
            
            # è·å–åˆ†é…ç»™è¿™ä¸ªä¸“å®¶çš„tokenç´¢å¼•
            exp_token_idx = token_idxs[start_idx:end_idx]
            
            # æå–è¿™äº›token
            expert_tokens = x[exp_token_idx]
            
            # è®©ä¸“å®¶å¤„ç†è¿™äº›token
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            
            # ä¹˜ä»¥å¯¹åº”çš„æƒé‡
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])
            
            # å°†ç»“æœç´¯åŠ åˆ°è¾“å‡ºç¼“å­˜çš„å¯¹åº”ä½ç½®
            # scatter_add_å®ç°äº†ç´¯åŠ æ“ä½œï¼ˆå› ä¸ºä¸€ä¸ªtokenå¯èƒ½è¢«å¤šä¸ªä¸“å®¶å¤„ç†ï¼‰
            expert_cache.scatter_add_(0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out)

        # è¿”å›ç´¯åŠ åçš„ç»“æœ
        return expert_cache


class MiniMindBlock(nn.Module):
    """
    MiniMindçš„Transformerå—
    æ¯ä¸ªå—åŒ…å«ä¸€ä¸ªè‡ªæ³¨æ„åŠ›å±‚å’Œä¸€ä¸ªå‰é¦ˆç½‘ç»œå±‚
    ä½¿ç”¨Pre-Normç»“æ„ï¼ˆåœ¨å­å±‚ä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–ï¼‰
    """
    def __init__(self, layer_id: int, config: MiniMindConfig):
        """
        åˆå§‹åŒ–Transformerå—
        å‚æ•°:
            layer_id: å½“å‰å±‚çš„IDï¼ˆä»0å¼€å§‹ï¼‰
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        # ä¿å­˜é…ç½®å‚æ•°
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        
        # è‡ªæ³¨æ„åŠ›å±‚
        self.self_attn = Attention(config)

        # å±‚ID
        self.layer_id = layer_id
        
        # æ³¨æ„åŠ›å±‚ä¹‹å‰çš„å½’ä¸€åŒ–å±‚ï¼ˆPre-Normï¼‰
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # å‰é¦ˆç½‘ç»œä¹‹å‰çš„å½’ä¸€åŒ–å±‚
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # å‰é¦ˆç½‘ç»œå±‚ï¼šæ ¹æ®é…ç½®é€‰æ‹©æ™®é€šFFNæˆ–MoE FFN
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        """
        Transformerå—çš„å‰å‘ä¼ æ’­
        ä½¿ç”¨Pre-Norm + æ®‹å·®è¿æ¥çš„ç»“æ„
        
        å‚æ•°:
            hidden_states: è¾“å…¥éšè—çŠ¶æ€
            position_embeddings: ä½ç½®ç¼–ç 
            past_key_value: KVç¼“å­˜
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            attention_mask: æ³¨æ„åŠ›æ©ç 
        è¿”å›:
            hidden_states: è¾“å‡ºéšè—çŠ¶æ€
            present_key_value: å½“å‰çš„KVç¼“å­˜
        """
        # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        residual = hidden_states
        
        # è‡ªæ³¨æ„åŠ›å­å±‚
        # 1. å…ˆè¿›è¡Œå½’ä¸€åŒ–ï¼ˆPre-Normï¼‰
        # 2. é€šè¿‡è‡ªæ³¨æ„åŠ›å±‚
        # 3. è¿”å›è¾“å‡ºå’ŒKVç¼“å­˜
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states),  # Pre-Norm
            position_embeddings,
            past_key_value,
            use_cache,
            attention_mask
        )
        
        # æ®‹å·®è¿æ¥ï¼šå°†æ³¨æ„åŠ›è¾“å‡ºä¸è¾“å…¥ç›¸åŠ 
        hidden_states += residual
        
        # å‰é¦ˆç½‘ç»œå­å±‚
        # 1. å…ˆè¿›è¡Œå½’ä¸€åŒ–ï¼ˆPre-Normï¼‰
        # 2. é€šè¿‡å‰é¦ˆç½‘ç»œ
        # 3. æ®‹å·®è¿æ¥ï¼šç›´æ¥åœ¨è¿™é‡Œç›¸åŠ 
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        
        # è¿”å›è¾“å‡ºå’ŒKVç¼“å­˜
        return hidden_states, present_key_value


class MiniMindModel(nn.Module):
    """
    MiniMindçš„ä¸»æ¨¡å‹ç±»
    åŒ…å«è¯åµŒå…¥å±‚ã€å¤šä¸ªTransformerå—å’Œæœ€ç»ˆçš„å½’ä¸€åŒ–å±‚
    """
    def __init__(self, config: MiniMindConfig):
        """
        åˆå§‹åŒ–MiniMindæ¨¡å‹
        å‚æ•°:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        super().__init__()
        self.config = config
        
        # ä¿å­˜è¯æ±‡è¡¨å¤§å°å’Œå±‚æ•°
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers
        
        # è¯åµŒå…¥å±‚ï¼šå°†token IDæ˜ å°„ä¸ºå‘é‡
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        
        # Dropoutå±‚ç”¨äºæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(config.dropout)
        
        # åˆ›å»ºå¤šä¸ªTransformerå—ï¼ˆæ¨¡å‹çš„ä¸»ä½“ï¼‰
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])
        
        # æœ€ç»ˆçš„å½’ä¸€åŒ–å±‚
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # é¢„è®¡ç®—RoPEä½ç½®ç¼–ç çš„coså’Œsinå€¼
        # è¿™äº›å€¼åœ¨æ•´ä¸ªè®­ç»ƒ/æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒä¸å˜ï¼Œæ‰€ä»¥é¢„å…ˆè®¡ç®—å¯ä»¥æé«˜æ•ˆç‡
        freqs_cos, freqs_sin = precompute_freqs_cis(
            dim=config.hidden_size // config.num_attention_heads,  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
            end=config.max_position_embeddings,  # æœ€å¤§åºåˆ—é•¿åº¦
            theta=config.rope_theta  # RoPEçš„thetaå‚æ•°
        )
        
        # å°†é¢„è®¡ç®—çš„å€¼æ³¨å†Œä¸ºbufferï¼ˆä¸æ˜¯å‚æ•°ï¼Œä½†ä¼šéšæ¨¡å‹ä¿å­˜/åŠ è½½ï¼‰
        # persistent=Falseè¡¨ç¤ºä¸ä¿å­˜åˆ°state_dictä¸­ï¼ˆå¯ä»¥é‡æ–°è®¡ç®—ï¼‰
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,  # è¾“å…¥çš„token ID
                attention_mask: Optional[torch.Tensor] = None,  # æ³¨æ„åŠ›æ©ç 
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,  # KVç¼“å­˜åˆ—è¡¨
                use_cache: bool = False,  # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
                **kwargs):  # å…¶ä»–å‚æ•°
        """
        æ¨¡å‹çš„å‰å‘ä¼ æ’­
        å‚æ•°:
            input_ids: è¾“å…¥token IDï¼Œå½¢çŠ¶ [batch_size, seq_length]
            attention_mask: æ³¨æ„åŠ›æ©ç 
            past_key_values: ä¹‹å‰çš„KVç¼“å­˜
            use_cache: æ˜¯å¦è¿”å›KVç¼“å­˜
        è¿”å›:
            hidden_states: æœ€ç»ˆçš„éšè—çŠ¶æ€
            presents: å½“å‰çš„KVç¼“å­˜åˆ—è¡¨
            aux_loss: MoEçš„è¾…åŠ©æŸå¤±
        """
        # è·å–è¾“å…¥å½¢çŠ¶
        batch_size, seq_length = input_ids.shape
        
        # å¦‚æœæ²¡æœ‰æä¾›KVç¼“å­˜ï¼Œä¸ºæ¯ä¸€å±‚åˆ›å»ºNone
        past_key_values = past_key_values or [None] * len(self.layers)
        
        # è®¡ç®—èµ·å§‹ä½ç½®ï¼ˆç”¨äºKVç¼“å­˜ï¼‰
        # å¦‚æœæœ‰ç¼“å­˜ï¼Œèµ·å§‹ä½ç½®æ˜¯ç¼“å­˜çš„é•¿åº¦ï¼›å¦åˆ™æ˜¯0
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        # è¯åµŒå…¥ï¼šå°†token IDè½¬æ¢ä¸ºå‘é‡ï¼Œç„¶ååº”ç”¨dropout
        hidden_states = self.dropout(self.embed_tokens(input_ids))

        # è·å–å½“å‰åºåˆ—å¯¹åº”çš„ä½ç½®ç¼–ç 
        # ä»start_poså¼€å§‹ï¼Œå–seq_lengthé•¿åº¦çš„ä½ç½®ç¼–ç 
        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        # å­˜å‚¨æ¯ä¸€å±‚çš„KVç¼“å­˜
        presents = []
        
        # ä¾æ¬¡é€šè¿‡æ¯ä¸ªTransformerå—
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            # é€šè¿‡å½“å‰å±‚
            hidden_states, present = layer(
                hidden_states,  # è¾“å…¥éšè—çŠ¶æ€
                position_embeddings,  # ä½ç½®ç¼–ç 
                past_key_value=past_key_value,  # è¯¥å±‚çš„KVç¼“å­˜
                use_cache=use_cache,  # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
                attention_mask=attention_mask  # æ³¨æ„åŠ›æ©ç 
            )
            # ä¿å­˜å½“å‰å±‚çš„KVç¼“å­˜
            presents.append(present)

        # æœ€ç»ˆå½’ä¸€åŒ–
        hidden_states = self.norm(hidden_states)

        # å¦‚æœä½¿ç”¨äº†MoEï¼Œæ”¶é›†æ‰€æœ‰MoEå±‚çš„è¾…åŠ©æŸå¤±
        # è¾…åŠ©æŸå¤±ç”¨äºå¹³è¡¡ä¸“å®¶è´Ÿè½½
        aux_loss = sum(
            layer.mlp.aux_loss  # è·å–æ¯ä¸ªMoEå±‚çš„è¾…åŠ©æŸå¤±
            for layer in self.layers
            if isinstance(layer.mlp, MOEFeedForward)  # åªå¤„ç†MoEå±‚
        )

        # è¿”å›æœ€ç»ˆéšè—çŠ¶æ€ã€KVç¼“å­˜åˆ—è¡¨å’Œè¾…åŠ©æŸå¤±
        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    """
    ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„MiniMindæ¨¡å‹
    è¿™æ˜¯å®Œæ•´çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«MiniMindModelå’Œè¯­è¨€æ¨¡å‹å¤´
    ç»§æ‰¿è‡ªPreTrainedModelä»¥å…¼å®¹transformersåº“
    ç»§æ‰¿è‡ªGenerationMixinä»¥æ”¯æŒæ–‡æœ¬ç”ŸæˆåŠŸèƒ½
    """
    # æŒ‡å®šé…ç½®ç±»
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig = None):
        """
        åˆå§‹åŒ–å› æœè¯­è¨€æ¨¡å‹
        å‚æ•°:
            config: æ¨¡å‹é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        self.config = config or MiniMindConfig()
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(self.config)
        
        # åˆ›å»ºMiniMindä¸»æ¨¡å‹
        self.model = MiniMindModel(self.config)
        
        # è¯­è¨€æ¨¡å‹å¤´ï¼šå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°çš„logits
        # ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªtoken
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        
        # æƒé‡å…±äº«ï¼šè®©è¯åµŒå…¥å±‚å’Œè¾“å‡ºå±‚å…±äº«æƒé‡
        # è¿™æ˜¯ä¸€ç§å¸¸è§çš„æŠ€å·§ï¼Œå¯ä»¥å‡å°‘å‚æ•°é‡å¹¶æé«˜æ€§èƒ½
        self.model.embed_tokens.weight = self.lm_head.weight
        
        # åˆ›å»ºè¾“å‡ºå¯¹è±¡ï¼ˆç”¨äºè¿”å›ç»“æœï¼‰
        self.OUT = CausalLMOutputWithPast()

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,  # è¾“å…¥token ID
                attention_mask: Optional[torch.Tensor] = None,  # æ³¨æ„åŠ›æ©ç 
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,  # KVç¼“å­˜
                use_cache: bool = False,  # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
                logits_to_keep: Union[int, torch.Tensor] = 0,  # ä¿ç•™å¤šå°‘ä¸ªä½ç½®çš„logits
                **args):  # å…¶ä»–å‚æ•°
        """
        æ¨¡å‹çš„å‰å‘ä¼ æ’­
        å‚æ•°:
            input_ids: è¾“å…¥token ID
            attention_mask: æ³¨æ„åŠ›æ©ç 
            past_key_values: KVç¼“å­˜
            use_cache: æ˜¯å¦è¿”å›KVç¼“å­˜
            logits_to_keep: ä¿ç•™å¤šå°‘ä¸ªä½ç½®çš„logitsï¼ˆç”¨äºèŠ‚çœå†…å­˜ï¼‰
                           å¦‚æœä¸º0ï¼Œä¿ç•™æ‰€æœ‰ä½ç½®
                           å¦‚æœä¸ºæ­£æ•´æ•°nï¼Œåªä¿ç•™æœ€ånä¸ªä½ç½®
        è¿”å›:
            CausalLMOutputWithPastå¯¹è±¡ï¼ŒåŒ…å«logitsã€hidden_statesã€past_key_valuesç­‰
        """
        # é€šè¿‡ä¸»æ¨¡å‹è·å–éšè—çŠ¶æ€
        h, past_kvs, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        
        # ç¡®å®šè¦ä¿ç•™å“ªäº›ä½ç½®çš„logits
        # å¦‚æœlogits_to_keepæ˜¯æ•´æ•°ï¼Œåˆ›å»ºä¸€ä¸ªåˆ‡ç‰‡å¯¹è±¡
        # ä¾‹å¦‚ï¼šlogits_to_keep=1 -> slice(-1, None) -> åªä¿ç•™æœ€åä¸€ä¸ªä½ç½®
        # å¦‚æœlogits_to_keepæ˜¯å¼ é‡ï¼Œç›´æ¥ä½¿ç”¨å®ƒä½œä¸ºç´¢å¼•
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        
        # é€šè¿‡è¯­è¨€æ¨¡å‹å¤´è®¡ç®—logits
        # åªå¯¹é€‰å®šçš„ä½ç½®è®¡ç®—logitsä»¥èŠ‚çœè®¡ç®—å’Œå†…å­˜
        logits = self.lm_head(h[:, slice_indices, :])
        
        # å°†ç»“æœå­˜å…¥è¾“å‡ºå¯¹è±¡
        self.OUT.__setitem__('last_hidden_state', h)  # æœ€åçš„éšè—çŠ¶æ€
        self.OUT.__setitem__('logits', logits)  # é¢„æµ‹çš„logits
        self.OUT.__setitem__('aux_loss', aux_loss)  # MoEçš„è¾…åŠ©æŸå¤±
        self.OUT.__setitem__('past_key_values', past_kvs)  # KVç¼“å­˜
        
        # è¿”å›è¾“å‡ºå¯¹è±¡
        return self.OUT
